---
title: "STAT 535 Final Project - Fall 2021"
author: 'Names: Said Arslan, Declan Gray-Mullen, and Rakeen Tanvir'
date: 'Due Date: December 15, 2021, 5:59pm'
output:
  html_document:
    toc: yes
    number_sections: yes
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(readr)
library(modelr)
library(countrycode)
library(ggplot2)
```

# Data collection

# Data preparation

## Import and transform into tidy data

```{r}
(ac <-
   read_csv("reference/african_crises.csv",
    col_types = cols(
        case = col_integer(),
        cc3 = col_factor(ordered = TRUE),
        country = col_character(),

        year = col_date(format = "%Y"),

        exch_usd = col_double(),
        gdp_weighted_default = col_double(),
        inflation_annual_cpi = col_double(),

        systemic_crisis = col_logical(),
        domestic_debt_in_default = col_logical(),
        sovereign_external_debt_default = col_logical(),
        independence = col_logical(),
        currency_crises = col_integer(),
        inflation_crises = col_logical(),

        banking_crisis = col_character()
    )
  )
  %>% mutate(banking_crisis = (banking_crisis == "crisis"))
  %>% mutate(currency_crises = (currency_crises > 0)) # assume 2==1 in 4 cases
)

assertthat::noNA(ac)
```

The data is tidy: Each variable has its own column, each observation has its own row, and each value has its own cell.

## The variables in `ac`

1. `case` is a factor variable that identifies the index for each country from the original, global dataset.  

2. `cc3` is a factor variable that identifies each country with a three letter code.

3. `country` is a factor variable that names each country.

4. `year` is a date variable that identifies the year of the observation.

5. `systemic_crisis` <logical> that identifies whether the observation is associated with a systemic crisis.

6. `exch_usd` <dbl>

7. `domestic_debt_in_default` <int>

8. `sovereign_external_debt_default` <int>

9. `gdp_weighted_default` <dbl>

10. `inflation_annual_cpi` <dbl>

11. `independence` <logical>

12. `currency_crises` <logical>

13. `inflation_crises` <logical>

14. `banking_crisis` <logical>


# Web Scraping: Commodity Import & Export Price Index
*Commodity of Terms of Trade: Commodity Import/Export Price Index, Individual Commodites Weighted by Ratio of Imports to GDP Historical, Annual (1965-present), Fixed Weights, Index (2012=100)*
```{r, include=FALSE}
ac_imf = ac
```
In order to expand the features of our dataset, we scrape additional data provided by the International Monetary Fund (IMF). While the IMF provides many formats including API REST endpoints, excel and csv file formats but we instead scrape the rendered html using their [online table viewer](https://data.imf.org/?sk=2CDDCCB8-0B59-43E9-B6A0-59210D5605D2&sId=1390030341854).

Using the viewer we selected the desired data and used inspect in Chrome to view the rendered html for the current page. We copied the div representing the entire table in html and pasted the text into local files.

First we wrote a function to convert the html, into a df matching the table viewer online.
```{r}
imf_html_to_df = function (html_filename){

  # we get the column labels by looking at the custom filename
  filename_pieces = str_split(html_filename, "_")[[1]]

  # read all the html in the file
  html_str = readLines(html_filename)

  # function returning the elements matched by the regex
  html_to_byRowList = function(full_file_str){
    regex_exp = "<div class=\"PPTSCellConText.*?>(.*?)<"
    result = stringr::str_match_all(full_file_str, regex_exp)
    return(result[[1]][, 2])
  }

  # all elements in our df by row
  elements = html_to_byRowList(html_str)

  # df 'schema' defined
  columns = c("country", seq(as.integer(filename_pieces[3]), as.integer(filename_pieces[4]), by=1))
  num_columns = length(columns)
  schema = matrix(ncol = num_columns, nrow = 0)
  df = data.frame(schema)
  colnames(df) = columns

  # grab 'inflation' elements, ignoring column headers
  elements = elements[num_columns:length(elements)]

  # populate df iterating through element by df row
  start = 1
  while(start + num_columns - 1 <= length(elements)){
    curr = elements[start:(start+num_columns-1)]
    curr = do.call(c, list(list(curr[1]), as.double(curr[2:(length(curr))])))
    df[nrow(df) + 1,] = curr
    start = start + num_columns
  }

  # add cc3 labels used in joining later
  df$cc3 = as_factor(countrycode(df$country, origin = 'country.name', destination = 'iso3c'))
  return(df)
}
```
```{r, include=FALSE}
export_df_1 = imf_html_to_df("reference/IMF_export_1965_1988_.html")
export_df_2 = imf_html_to_df("reference/IMF_export_1989_2014_.html")
export_df_3 = imf_html_to_df("reference/IMF_export_2015_2020_.html")
```

We then used that function to generate df's for each 20 year grouping and join them together final IMF df (since at most 20 columns render)!
```{r}
export_df = merge(x=merge(x=export_df_1, y=export_df_2, by=c("cc3", "country")), y=export_df_3, by=c("cc3", "country"))
as_tibble(export_df)
```

Lastly we need to join this data to our main ac data. Two complexities:
1. reformatting the year columns as a single new 'year' column
2. join on year & cc3 code, since Ivory Coast has different names

Write a function to transform the IMF df into a 'join-compatible' format:
```{r}
imf_df_transform = function(df, col_name){
  # setup
  df = df %>% arrange(cc3)

  # select each grouping of data we need
  just_cpi_df = subset(df, select = -c(cc3, country))
  cpi = unlist(just_cpi_df, use.names=FALSE)
  cc3 = unlist(subset(df, select = c(cc3)), use.names=FALSE)
  country = unlist(subset(df, select = c(country)), use.names=FALSE)
  year = colnames(just_cpi_df)

  # helpers for reconstruction
  row_repeats = length(cpi) / length(cc3)
  col_repeats = length(cpi) / length(year)
  as.date=function(x){
    year_starts = paste(as.character(x), rep("01", length(x)), rep("01", length(x)), sep="-" )
    return(as.Date(year_starts, format="%Y-%m-%d"))
  }

  # reconstruct the joinable df
  df_joinable = data.frame(
    country=rep(country, times=row_repeats),
    cc3=rep(cc3, times=row_repeats),
    year=rep(as.date(year),each=col_repeats)
  )
  df_joinable[col_name] = cpi

  # we remove the country column since we dont need for join
  df_joinable = subset (df_joinable, select = -country)
  return(df_joinable)
}
```

Lastly, we use this function and inner join the new data to ac!
```{r}
export_df_joinable = imf_df_transform(export_df, "inflation_annual_epi")
ac_imf = inner_join(ac_imf, export_df_joinable, by = c("cc3", "year"))
```

We can then repeat the scraping process to obtain import price data:
```{r, include=FALSE}
import_df_1 = imf_html_to_df("reference/IMF_import_1965_1990_.html")
import_df_2 = imf_html_to_df("reference/IMF_import_1991_2016_.html")
import_df_3 = imf_html_to_df("reference/IMF_import_2017_2020_.html")
```
```{r}
import_df = merge(x=merge(x=import_df_1, y=import_df_2, by=c("cc3", "country")), y=import_df_3, by=c("cc3", "country"))
import_df = import_df %>% arrange(cc3)
import_df_joinable = imf_df_transform(import_df, "inflation_annual_ipi")

ac_imf = inner_join(ac_imf, import_df_joinable, by = c("cc3", "year"))
as_tibble(ac_imf %>% select(country, year, inflation_annual_cpi, inflation_annual_epi, inflation_annual_ipi))
```
```{r, include=FALSE}
rm(export_df_1, export_df_2, export_df_3, import_df_1, import_df_2, import_df_3)
```

other ideas: rainfall, almanac/climate date

### Transform Index Annual to Index Percentage Change
```{r}
ac_imf = ac_imf %>% group_by(cc3) %>% arrange(year) %>%
  mutate(inflation_change_cpi=(inflation_annual_cpi-lag(inflation_annual_cpi))/lag(inflation_annual_cpi)*100) %>%
  mutate(inflation_change_epi=(inflation_annual_epi-lag(inflation_annual_epi))/lag(inflation_annual_epi)*100) %>%
  mutate(inflation_change_ipi=(inflation_annual_ipi-lag(inflation_annual_ipi))/lag(inflation_annual_ipi)*100) %>% ungroup()

# remove the undefined first year values
ac_imf = ac_imf %>% filter(!is.na(inflation_change_cpi))

# remove the one Inf tuple :(
ac_imf = ac_imf %>% filter(!(inflation_change_cpi==Inf))

# show the new data!
as_tibble(ac_imf %>% arrange(cc3, year) %>% select(cc3, year, inflation_annual_cpi, inflation_change_cpi, inflation_annual_epi, inflation_change_epi, inflation_annual_ipi, inflation_change_ipi))
```


# Exploratory Data Analysis

```{r}
names(ac)
```

```{r}
# Exploratory Data (ed) set
ed_orig <- ac %>%

  select(c(cc3, country, year, case, systemic_crisis, banking_crisis, currency_crises, inflation_crises, independence, domestic_debt_in_default, sovereign_external_debt_default, inflation_annual_cpi, exch_usd)) %>%

  mutate_at(
    vars(systemic_crisis, banking_crisis, currency_crises, inflation_crises, independence, domestic_debt_in_default, sovereign_external_debt_default),
    as.integer) %>%

  data.frame()

ed <- ed_orig[ , -c(1:3) ]
```



```{r}
head(ed)
```

```{r}
summary(ed)
```

## Data visualization

### Histograms

Visualize spread of the distribution of numeric variables.

```{r}
# par(mfrow=c(1,(ncol(ed)-3))) # Error in plot.new() : figure margins too large
for(i in 1:ncol(ed)) {
    hist(ed[,i], main=names(ed)[i])
}
```

Crises and debt default do not happen on most years. Most of the observations are associated with years of independence. The exchange rate to USD and annual CPI inflation are highly right-skewed with very outlier values. These may have something to do with the crises and debt defaults.

### Box and Whisker

Easily identify outliers

```{r}
# par(mfrow=c(1,(ncol(ed)-3))) # Error in plot.new() : figure margins too large
for(i in 1:ncol(ed)) {
    boxplot(ed[,i], main=names(ed)[i])
}
```

While USD exchange rate seems to have many data points in its right-skewed tail, the annual CPI inflation seems to have a single or small number with a very large value, which may indicate an error or an outlier in the data. We may wish to remove this from the data.  

```{r}
# ed <- ed[-(which.max(ed$inflation_annual_cpi)), ]
```


### Correlation

```{r}
library(corrplot)
(correlations <- cor(ed))
corrplot(correlations, method="circle")
```

The variables of interest all have non-negative correlation.

The strongest correlation is between systemic crises and banking crises, as expected. There are moderate correlation between sovereign external and domestic debt defaults (unsurprising), between USD exchange rate and sovereign external debt default (also unsurprising), and between inflation and currency crises (and again).

To increase confidence in our prediction of systemic crises, it may be useful to remove a predictor variable from each pair. We could remove sovereign external debt default or we could remove both domestic debt default and USD exchange rate. We can also remove currency crises since inflation crises are more strongly correlated with systemic crises than are currency crises, though both are weakly correlated with systemic crises.

In addition, banking crises are weakly correlated with both external and domestic debt defaults as well as inflation crises. Since banking crisis is strongly correlated with systemic crises, perhaps most of the information we would get from the default and crises data may already be contained in banking crisis.

### Scatterplot

```{r}
pairs(ed, col=ed$systemic_crisis)
```

It is difficult to make out much from this visualization.

```{r}
plt <- ggplot(data = ed, mapping = aes(y = systemic_crisis))

plt +
  geom_jitter(mapping = aes(x = banking_crisis), width = .25, height = .25)
```

As expected, most of the data is not associated with crises, but when they are, banking crises and systemic crises usually occur together.

```{r}
# install.packages("caret")
# library(caret)
# x <- ed[,3:10]
# y <- ed[,2]
# scales <- list(x=list(relation="free"), y=list(relation="free"))
# featurePlot(x=x, y=y, plot="density", scales=scales)
```

```{r}

(year_table <- ac %>%
  group_by(country) %>%
  summarize(first_year = min(year), count = n() ) %>%
  arrange(first_year) %>%
  mutate(prop = count/sum(count))
)

(indep_table <- ac %>%
  filter(independence == 1) %>%
  group_by(country) %>%
  summarize(indep_year = min(year), count = n() ) %>%
  arrange(indep_year) %>%
  mutate(prop = count/sum(count))
)

(sys_crisis_table <- ac %>%
  filter(systemic_crisis == 1) %>%
  group_by(country) %>%
  summarize(earliest_sys_crisis = min(year), count = n() ) %>%
  arrange(earliest_sys_crisis) %>%
  mutate(prop = count/sum(count))
)

plt1 <- ggplot(data = filter(ac, systemic_crisis == TRUE))

plt1 +
  geom_jitter(mapping = aes(x = year, y = country),
             color = "blue", alpha = .75, width = .1, height = .1) +
  geom_jitter(data = indep_table,
             mapping = aes(x = indep_year, y = country),
             color = "black", alpha = .75, width = .1, height = .1) +
  geom_jitter(data = year_table,
             mapping = aes(x = first_year, y = country),
             color = "red", alpha = .75, width = .1, height = .1)

# Add banking crises and other explanatory variables of interest
```

The leftmost red points indicate the first year of the data. The black points represent the first year of independence. The blue points indicate occurrences of systemic crises.

Most countries achieved independence by 1965. The four exceptions are Algeria (1968) and Angola (1975) on the later end and Egypt (1860) and South Africa (1910) on the earlier end. Consequently, Algeria and Angola have less filtered observations for analysis, while Egypt and South Africa alone would make up a large proportion of the data used for analysis. The four countries combined makeup almost about 40% of the data, so let's use the proportion sizes to help us decide how to split the data for training/exploration and validation (query and test).

Only 82 total systemic crises in data, with the earliest occurring in Algeria in 1870 and in Egypt in 1907. Perhaps we should focus on analysis on the more recent period from 1976 and on, which also coincides with the time period when all countries had achieved independence, in order to train the model on data characterized by recent economic conditions under independence.

# Preliminary model investigation

## Simple Logistic Regression (SLR) Model

The logistic response function:

```{r}
# Logistic response function
pi_i <- function(B0, B1, X) {
  return ( 1 / ( 1 + exp( - (B0 + B1 * X) ) ) )
}
```

The probability distribution for an observation is simply the probability mass function of a Bernoulli random variable. And since the Yi observations are assumed to be independent, their joint probability function is the likelihood.

```{r}

distr_i <- function(B0, B1, X, Y) {
  probs <- pi_i(B0, B1, X)
  return ( probs^Y * (1-probs)^(1-Y) )
}

likelihood <- function(B0, B1, X, Y) {
  return ( prod( distr_i(B0, B1, X, Y) ) )
}
```

However, it is numerically advantageous to find the maximum likelihood estimates by working with the logarithm of the joint probability function rather than the actual values which tend to be very close to zero and therefore prone to errors of computational precision. Therefore, the objective function to minimize is the log-likelihood:

```{r}

log_likelihood <- function(B0, B1, X, Y) {
  return ( sum( Y * log(pi_i(B0, B1, X))  + ( 1 - Y ) * log( 1 - pi_i(B0, B1, X) ) ) )
}
```

Create a helper function to use in the `optim()` function:

```{r}
ll_helper <- function(params) {
  return( -log_likelihood(params[1], params[2], X = X, Y = Y) )
  # Take the negative of the log-likelihood since we want to maximize this value but the optim() function minimizes by default.
}
```

Obtain point estimates of the coefficients of a simple logistic regression where the fitting is based on maximizing the log-likelihood of the data.

```{r}

X = ac$systemic_crisis
Y = ac$banking_crisis
params <- c(0, 0)
(best <- optim(par = params, fn = ll_helper))

probs <- pi_i(best$par[1], best$par[2], X)
cutoff <- mean(unique(probs)) # only two values
preds <- ifelse(probs > cutoff, TRUE, FALSE)
correct <- preds == Y

paste("Training Accuracy", mean(correct))
paste("Sensitivity", mean(preds[Y==1] == Y[Y==1]))
paste("Sensitivity", mean(preds[Y==0] == Y[Y==0]))
paste("Correctly predicted",
  sum(preds & correct),
  "of",
  sum(Y),
  "crises"
)
paste("False positives",
  sum(preds & !(correct))

)
```

## Compare to built-in function for Logistic Regression as a family of the Generalized Linear Model

```{r}
glm_fit = glm(formula = Y ~ X, family = binomial(link = "logit"))

glm_probs <- predict(glm_fit, newdata = data.frame(Y, X), type = "response")

glm_preds <- ifelse(glm_probs > mean(unique(glm_probs)), TRUE, FALSE)

glm_correct <- glm_preds == Y

paste("Training Accuracy", mean(glm_correct))
paste("Sensitivity", mean(preds[Y==1] == Y[Y==1]))
paste("Sensitivity", mean(preds[Y==0] == Y[Y==0]))
paste("Correctly predicted",
  sum(glm_preds & glm_correct),
  "of",
  sum(Y),
  "crises"
)
paste("False positives",
  sum(glm_preds & !(glm_correct))
)
```
The default method "glm.fit" uses iteratively reweighted least squares (IWLS).

## Compare likelihood from custom SLR vs. built-in GLM

```{r}
(slr_ll <- log_likelihood(best$par[1], best$par[2], X, Y))
(glm_ll <- log_likelihood(glm_fit$coefficients[1],    
                          glm_fit$coefficients[2], X, Y) )
```

The optimization procedure provides us with point estimates, but we have no measure of uncertainty for them: neither a standard deviation nor confidence intervals to determine whether or not they are different from 0 with any statistical certainty. The bootstrap method allows us to do so.

## Bootstrap confidence intervals

Construct a bootstrap sample for the estimated coefficients of a simple logistic regression where the fitting is based on maximizing the log-likelihood.

```{r}
n <- nrow(Y)
M <- 1e4
b_params <- c(0, 0)
# fit_bstp <- vector(mode = "list", length = M)
b0 <- numeric(M)
b1 <- numeric(M)

max_ll_regression_helper_bstp <- function(b) {
  return( -log_likelihood(b[1], b[2], X = X, Y = Y) )
}

for (m in 1:M) {
  bstp <- sample(n, replace = TRUE)
  X <- X[bstp]
  Y <- Y[bstp]
  fit_bstp <- optim(b_params, max_ll_regression_helper_bstp)
  b0[m] <- fit_bstp$par[1]
  b1[m] <- fit_bstp$par[2]
}
```


## c. Estimate the standard deviations of each of the coefficients.

```{r}
b0_summary <- summary(b0)
b0_samp_var <- sum( (b0 - b0_summary["Mean"])^2 ) / ( length(b0) - 1 )
(b0_std_dev <- sqrt(b0_samp_var))
sd(b0)
```

```{r}
b1_summary <- summary(b1)
b1_samp_var <- sum( (b1 - b1_summary["Mean"])^2 ) / ( length(b1) - 1 )
(b1_std_dev <- sqrt(b1_samp_var))
sd(b1)
```


## d. Construct 95% confidence intervals for the coefficients and, based on those, determine whether or not we can reject a null hypothesis that each of them is equal to zero.

```{r}
alpha <- .05
probs <- c(alpha/2, 1 - alpha/2)
```

```{r}
( b0_empirical_bootstrap_CI <- quantile(b0, probs) )

ifelse(b0_empirical_bootstrap_CI[1] < 0 & 0 < b0_empirical_bootstrap_CI[2], "Fail to reject null", "Reject null")

ggplot(data.frame(x = b0)) +
  geom_histogram(aes(x = x, y = ..density..), color = "white") +
  geom_vline(xintercept = b0_empirical_bootstrap_CI, col = "blue")

```


```{r}
( b1_empirical_bootstrap_CI <- quantile(b1, probs) )

ifelse(b1_empirical_bootstrap_CI[1] < 0 & 0 < b1_empirical_bootstrap_CI[2], "Fail to reject null", "Reject null")


ggplot(data.frame(x = b1)) +
  geom_histogram(aes(x = x, y = ..density..), color = "white") +
  geom_vline(xintercept = b1_empirical_bootstrap_CI, col = "blue")
```


## Multiple Logistic Regression (MLR) Model

...Bootstrap confidence interval
compare to glm results

# General Linear Model Regression

```{r}
ac_imf = ac_imf %>% mutate(banking_crisis=as.factor(banking_crisis))
glm.fit = glm(
  formula = banking_crisis ~ inflation_annual_cpi + inflation_annual_epi + inflation_annual_ipi,
  family = binomial,
  data = ac_imf
)
glm.probs <- predict(glm.fit, newdata = ac_imf, type = "response")
glm.pred <- ifelse(glm.probs > 0.5, TRUE, FALSE)

print_accuracy_metrics = function(pred, trues){
  lr_got_it_right = pred == trues
  out = paste("Training Accuracy", mean(lr_got_it_right))
  out = c(out, paste("Correctly predicted",
    sum(pred & lr_got_it_right),
    "of",
    sum(trues),
    "crises"
  ))
  out = c(out, paste("False positives",
    sum(pred & !(lr_got_it_right))
  ))
  return(out)
}
print_accuracy_metrics(glm.pred, as.logical(ac_imf$banking_crisis))
```
This approach has a major flaw. Since the absolute inflation index values are dependent on the year, all this is telling us is later years are more likely to be a crises. We can switch to using the relative percent change instead of the aboslute value to make the input data time-series-independent, and performance takes a massive hit.

```{r}
glm.fit = glm(
  formula = banking_crisis ~  inflation_change_cpi + inflation_change_epi + inflation_change_ipi,
  family = binomial,
  data = ac_imf
)
glm.probs <- predict(glm.fit, newdata = ac_imf, type = "response")
glm.pred <- ifelse(glm.probs > 0.5, TRUE, FALSE)

print_accuracy_metrics(glm.pred, as.logical(ac_imf$banking_crisis))
```
