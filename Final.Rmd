---
title: "STAT 535 Final Project - Fall 2021"
author: 'Names: Said Arslan, Declan Gray-Mullen, and Rakeen Tanvir'
date: 'Due Date: December 15, 2021, 5:59pm'
output:
  html_document:
    toc: yes
    number_sections: yes
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(readr)
library(modelr)
library(countrycode)
```

# Completed Tasks:

Install GitHub and GitHub Desktop App

Share repo on GitHub to usernames: graydh, arslansaid

# To do

Sampling - empirical data vs. theoretical distribution using chi-square goodness of fit tests, bootstrapping
Monte Carlo - homework #2
  Identify distribution for data
  Investigate whether t-test is appropriate with power study
  Determine confidence interval for variable of interest
Bootstrapping - quantify uncertainty with confidence interval of point estimate (double numeric predictor variables)
Difference in differences?

Interaction/effects of economic variables?

Regression - robust, logistic, interaction

At which annual rate of inflation does an Inflation Crisis become a practical certainty?

# Data collection and preparation

## Data collection

### What type of observational study are we conducting? 

Confirmatory - confirm or not confirm hypothesis derived from previous studies or hunches

Exploratory - search for explanatory variables that might be related to the response variable

If confirmatory, which explanatory variables are:

Primary variables - involved in the hypothesis

Control variables - included to reflect existing knowledge by accounting for known influences on the response variable

Acknowledgements

Reinhart, C., Rogoff, K., Trebesch, C. and Reinhart, V. (2019) Global Crises Data by Country.
[online] https://www.hbs.edu/behavioral-finance-and-financial-stability/data. Available at: https://www.hbs.edu/behavioral-finance-and-financial-stability/data/Pages/global.aspx [Accessed: 17 July 2019].

On this page we present data collected over many years by Carmen Reinhart (with her coauthors Ken Rogoff, Christoph Trebesch, and Vincent Reinhart). These include Banking Crisis dates for more than 70 countries from 1800-present, exchange rate crises, stock market crises, sovereign debt growth and default, and many other data series. 

To see Carmenâ€™s related research, please visit her website here:

https://carmenreinhart.com/debt-and-debt-crises/

https://carmenreinhart.com/wp-content/uploads/2020/02/229_data.pdf


The BFFS Project keeps this data updated and available for download. Please email us with any questions.

Inspiration

My inspiration stems from two questions: "Which factors are most associated with Systemic Crises in Africa?" And; "At which annual rate of inflation does an Inflation Crisis become a practical certainty?"

## Data preparation

### Import and transform into tidy data

```{r}
(ac <- 
   read_csv("reference/african_crises.csv", 
    col_types = cols(
        case = col_integer(), 
        cc3 = col_factor(ordered = TRUE), 
        country = col_character(),
        
        year = col_date(format = "%Y"),
        
        exch_usd = col_double(),
        gdp_weighted_default = col_double(), 
        inflation_annual_cpi = col_double(),
        
        systemic_crisis = col_logical(), 
        domestic_debt_in_default = col_logical(), 
        sovereign_external_debt_default = col_logical(), 
        independence = col_logical(), 
        currency_crises = col_integer(), 
        inflation_crises = col_logical(),
        
        banking_crisis = col_character()
    )
  ) 
  %>% mutate(banking_crisis = (banking_crisis == "crisis"))
  %>% mutate(currency_crises = (currency_crises > 0)) # assume 2==1 in 4 cases
)

assertthat::noNA(ac)
```
```{r, include=FALSE}
df = ac

df_test <- filter(df, (systemic_crisis == FALSE) && (banking_crisis == TRUE))
```

Bootstrap confidence interval for regression coefficients (HW10)

The data is tidy: Each variable has its own column, each observation has its own row, and each value has its own cell.

### The variables in `ac`

1. `case` is a factor variable that identifies the index for each country from the original, global dataset.  

2. `cc3` is a factor variable that identifies each country with a three letter code.

3. `country` is a factor variable that names each country.

4. `year` is a date variable that identifies the year of the observation.

5. `systemic_crisis` <logical> that identifies whether the observation is associated with a systemic crisis. 

6. `exch_usd` <dbl>

7. `domestic_debt_in_default` <int>

8. `sovereign_external_debt_default` <int>

9. `gdp_weighted_default` <dbl>

10. `inflation_annual_cpi` <dbl>

11. `independence` <logical>

12. `currency_crises` <logical>

13. `inflation_crises` <logical>

14. `banking_crisis` <logical>

How are the different crises related? Heat map, refine question. Reverse engineering to identify motivation for compiling the data set. Investigate relevant research and economic definitions. 

### View the factor variables with plots and a tibble

```{r}
plt <- ggplot(data = df)
plt + geom_bar(mapping = aes(x = case))
plt + geom_bar(mapping = aes(x = cc3))
plt + geom_bar(mapping = aes(x = country)) +
  theme(axis.text.x=element_blank())

(factors <- unique(df[ , 1:3]) )

```

### Focus on years following independence. 

```{r}
plt + 
  geom_point(mapping = aes(x = year, y = independence, color = country))

(indep_table <- df %>% 
  filter(independence == 1) %>%  
  group_by(country) %>% 
  summarize(indep_year = min(year), count = n() ) %>% 
  arrange(indep_year) %>% 
  mutate(prop = count/sum(count))
)

# Add date of independence


```

Most countries achieved independence by 1965. The four exceptions are Algeria (1968) and Angola (1975) on the later end and Egypt (1860) and South Africa (1910) on the earlier end. Consequently, Algeria and Angola have less filtered observations for analysis, while Egypt and South Africa alone would make up a large proportion of the data used for analysis. The four countries combined makeup almost about 40% of the data, so let's use the proportion sizes to help us decide how to split the data for training/exploration and validation (query and test).

```{r}
plt + 
  geom_point(mapping = aes(x = year, y = systemic_crisis, color = country))

# Jitter or predetermined distances
# TRUE values only
# ideological basis

(sys_crisis_table <- df %>% 
  filter(systemic_crisis == 1) %>%  
  group_by(country) %>% 
  summarize(earliest_sys_crisis = min(year), count = n() ) %>% 
  arrange(earliest_sys_crisis) %>% 
  mutate(prop = count/sum(count))
)
```

Only 82 total systemic crises in data, with the earliest occurring in Algeria in 1870 and in Egypt in 1907. Perhaps we should focus on analysis on the more recent period from 1976 and on, which also coincides with the time period when all countries had achieved independence, in order to train the model on data characterized by recent economic conditions under independence.

Combine independence and systemic crises into one plot. Identify marker for ind per country on its own line and then its systemic crises. Is systemic crisis associated with xxx? Demonstrate simple relationships with scatter plots and regression. That indicates hypothesis and additional variables to collect/create.Break out by countries.

Simpson's Paradox?
Changes in regression relationships
What variables should be thrown away?
Repeat analysis for variable is T or F, or in or out of model.
Plots + confidence intervals


### Split the data into three pieces

~60% of the data goes into a training (or exploration) set

~20% goes into a test set used to compare models by hand

~20% is held back for a validation set to test the final model

What split decision rule should we follow?

1. Split by country.

2. Randomly select observations from the four highlighted countries to split among the query and test sets. 

3. Randomly select observations from the entire data set to split among the training, query, and test sets. 

Because we are working with time series data, let's maintain continuity over time and split the data by country.

```{r}
train_df <- NA
```


### Web Scraping: Commodity Import & Export Price Index

From Commodity of Terms of Trade: Commodity Import/Export Price Index, Individual Commodites Weighted by Ratio of Imports to GDP Historical, Annual (1965-present), Fixed Weights, Index(2012=100)
```{r}
ac_imf = ac
```
In order to expand the features of our dataset, we scrape additional data provided by the International Monetary Fund (IMF). While the IMF provides many formats including API REST endpoints, excel and csv file formats but we instead scrape the rendered html using their [online table viewer](https://data.imf.org/?sk=2CDDCCB8-0B59-43E9-B6A0-59210D5605D2&sId=1390030341854).

Using the tool we selected the data desired and used inspect element to view the html source for the current page view. We then copied the div representing the entire table in html and pasted the text into local files.

```{r}
imf_html_to_df = function (html_filename){
  filename_pieces = str_split(html_filename, "_")[[1]]
  
  html_str = readLines(html_filename)
  html_to_byRowList = function(full_file_str){
    regex_exp = "<div class=\"PPTSCellConText.*?>(.*?)<"
    result = stringr::str_match_all(full_file_str, regex_exp)
    return(result)
  }
  
  # by row vector of all elements in our df
  elements = html_to_byRowList(html_str)[[1]][, 2] 
  
  # df 'schema' defined
  columns = c("country", seq(as.integer(filename_pieces[3]), as.integer(filename_pieces[4]), by=1))
  num_columns = length(columns)
  schema = matrix(ncol = num_columns, nrow = 0)
  df = data.frame(schema)
  colnames(df) = columns
  
  elements = elements[num_columns:length(elements)] # first elements are columns year labels (missing 'country')
  
  # populate df iterating through elements
  start = 1
  while(start + num_columns - 1 <= length(elements)){
    curr = elements[start:(start+num_columns-1)]
    curr = do.call(c, list(list(curr[1]), as.double(curr[2:(length(curr))])))
    df[nrow(df) + 1,] = curr
    start = start + num_columns
  }
  df$cc3 = as_factor(countrycode(df$country, origin = 'country.name', destination = 'iso3c')) 
  return(df)
}

export_df_1 = imf_html_to_df("reference/IMF_export_1965_1988_.html")
```
```{r, include=FALSE}
export_df_2 = imf_html_to_df("reference/IMF_export_1989_2014_.html")
export_df_3 = imf_html_to_df("reference/IMF_export_2015_2020_.html")
```
```{r}
export_df = merge(x=merge(x=export_df_1, y=export_df_2, by=c("cc3", "country")), y=export_df_3, by=c("cc3", "country"))
export_df = export_df %>% arrange(cc3)
as_tibble(export_df)
```

Transform the IMF  into a 'join compatable' data frame.
```{r}
imf_df_transform = function(df, col_name){
  just_cpi_df = subset(df, select = -c(cc3, country))
  cpi = unlist(just_cpi_df, use.names=FALSE)
  cc3 = unlist(subset(df, select = c(cc3)), use.names=FALSE)
  country = unlist(subset(df, select = c(country)), use.names=FALSE)
  year = colnames(just_cpi_df)
  row_repeats = length(cpi) / length(cc3)
  col_repeats = length(cpi) / length(year)
  as.date=function(x){
    year_starts = paste(as.character(x), rep("01", length(x)), rep("01", length(x)), sep="-" )
    return(as.Date(year_starts, format="%Y-%m-%d"))
  }
  df_joinable = data.frame(country=rep(country, times=row_repeats), cc3=rep(cc3, times=row_repeats), year=rep(as.date(year), each=col_repeats))
  df_joinable[col_name] = cpi
  df_joinable = subset (df_joinable, select = -country)
  return(df_joinable)
}
export_df_joinable = imf_df_transform(export_df, "inflation_annual_epi")
```
Lastly, we inner join the two tables!
```{r}
ac_imf = inner_join(ac_imf, export_df_joinable, by = c("cc3", "year"))
```


We then repeat the whole process to process webpages containing import price data
```{r}
import_df_1 = imf_html_to_df("reference/IMF_import_1965_1990_.html")
```
```{r, include=FALSE}
import_df_2 = imf_html_to_df("reference/IMF_import_1991_2016_.html")
import_df_3 = imf_html_to_df("reference/IMF_import_2017_2020_.html")
```
```{r}
import_df = merge(x=merge(x=import_df_1, y=import_df_2, by=c("cc3", "country")), y=import_df_3, by=c("cc3", "country"))
import_df = import_df %>% arrange(cc3)
as_tibble(import_df)

import_df_joinable = imf_df_transform(import_df, "inflation_annual_ipi")

# dont run this more than once 
ac_imf = inner_join(ac_imf, import_df_joinable, by = c("cc3", "year"))

as_tibble(ac_imf %>% select(country, year, inflation_annual_cpi, inflation_annual_epi, inflation_annual_ipi))
```

other ideas: rainfall, almanac/climate date

### Transform Index Annual to Index Percentage Change
```{r}
ac_imf = ac_imf %>% group_by(cc3) %>% arrange(year) %>% 
  mutate(inflation_change_cpi=(inflation_annual_cpi-lag(inflation_annual_cpi))/lag(inflation_annual_cpi)*100) %>%
  mutate(inflation_change_epi=(inflation_annual_epi-lag(inflation_annual_epi))/lag(inflation_annual_epi)*100) %>%
  mutate(inflation_change_ipi=(inflation_annual_ipi-lag(inflation_annual_ipi))/lag(inflation_annual_ipi)*100) %>% ungroup()

ac_imf = ac_imf %>% filter(!is.na(inflation_change_cpi))

as_tibble(ac_imf %>% arrange(cc3, year) %>% select(cc3, year, inflation_annual_cpi, inflation_change_cpi, inflation_annual_epi, inflation_change_epi, inflation_annual_ipi, inflation_change_ipi))
```

## Preliminary checks on data quality

### Edit checks and plots to identify gross data errors and extreme outliers

## Preliminary model investigation

### Simple Logistic Regression Model

```{r}
ac_matrix <- as.matrix(ac[5:ncol(ac)])
Y <- ac_matrix[ , "systemic_crisis"]
X <- cbind( rep(1, nrow(ac_matrix)), ac_matrix[ , 2:ncol(ac_matrix)] )

```


```{r}
likelihood <- function(B0, B1, X) {
  return ( ( 1 + exp( -t(X) )^(-1) ) )
}
```

```{r}
log_likelihood <- function(B0, B1, X, Y) {
  return ( sum( Y * log( likelihood(B0, B1, X) )  + ( length(Y) - sum(Y) ) * log( 1 - likelihood(B0, B1, X) ) ) )
}
```

```{r}
ll_data <- function(B0, B1) {
  return( log_likelihood(B0, B1, X = X, Y = Y) )
}
```

```{r}
ll_helper <- function(params) {
  return ( -ll_data(params[1], params[2]) )
}
```

Generate test data:

```{r}
n <- 1e4
Y <- rep( rbernoulli(n, p = .1) )
X <- Y + .1

params <- c(.5, .5)

# (best <- optim(par = params, fn = ll_helper, method = "L-BFGS-B") )
```

glm - family: binomial
Continue to expand...
...Bootstrap confidence interval
compare to glm results

```{r}
# some percents are Inf, can't perform regression
# assertthat::assert_that(all(is.finite(ac_imf$inflation_change_cpi)))
# ac_imf %>% filter(inflation_change_cpi==Inf)

# but we can perform logistic regression on the annual values
ac_imf = ac_imf %>% mutate(banking_crisis=as.factor(banking_crisis))
glm.fit = glm(formula = banking_crisis ~ inflation_annual_cpi + inflation_annual_epi + inflation_annual_ipi, family = binomial, data = ac_imf)

glm.probs <- predict(glm.fit, newdata = ac_imf, type = "response")

glm.pred <- ifelse(glm.probs > 0.5, TRUE, FALSE)

lr_got_it_right = glm.pred == as.logical(ac_imf$banking_crisis)

paste("Training Accuracy", mean(lr_got_it_right))
paste("Correctly predicted",
  sum(glm.pred & lr_got_it_right),
  "of",
  sum(as.logical(ac_imf$banking_crisis)),
  "crises"
)
paste("False positives",
  sum(glm.pred & !(lr_got_it_right))
)
```


In order to potentially capture any non-linearity in the data; we can apply a basis expansion and see if it improves performance.
```{r}
#library(grpreg)
#Data <- gen_nonlinear_data(n=1000)
input_features = ac_imf %>% select(inflation_annual_cpi, inflation_annual_epi, inflation_annual_ipi)
nrow = length(input_features$inflation_annual_cpi)
data = as.numeric(as.matrix(input_features))
X = matrix(data, ncol=3L, nrow=nrow)
X <- grpreg::expand_spline(X, df=6)$X
X = as.data.frame(X)
X$banking_crisis = ac_imf$banking_crisis

glm.fit = glm(formula = banking_crisis ~ V1_1 + V1_2 + V1_3 + V1_4 + V1_5 + V1_6 + V2_1 + V2_2 + V2_3 + V2_4 + V2_5 + V2_6 + V3_1 + V3_2 + V3_3 + V3_4 + V3_5 + V3_6, family = binomial, data = X)
glm.probs <- predict(glm.fit, newdata = X, type = "response")
glm.pred <- ifelse(glm.probs > 0.5, TRUE, FALSE)

lr_got_it_right = glm.pred == as.logical(ac_imf$banking_crisis)
paste("Training Accuracy", mean(lr_got_it_right))
paste("Correctly predicted",
  sum(glm.pred & lr_got_it_right),
  "of",
  sum(as.logical(ac_imf$banking_crisis)),
  "crises"
)
paste("False positives",
  sum(glm.pred & !(lr_got_it_right))
)



```
While we are able to predict 1 more crisis accurately, we also have 1 more false positive.


### Diagnostics for relationships and strong interactions

### Apply remedial measures, if needed


# Reduction of number of explanatory variables (for exploratory observational studies)

Not required for confirmatory observational studies. 

## Determine several potentially useful subsets of explanatory variables; include known essential variables


# Model refinement and selection

## Investigate curvature and interaction effects more fully

## Study residuals and other diagnostics

## Apply remedial measures, if needed

## Select tentative model


# Model validation - 2-3 data sets

## Validity checks

## Final regression model