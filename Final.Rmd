---
title: "STAT 535 Final Project - Fall 2021"
author: 'Names: Said Arslan, Declan Gray-Mullen, and Rakeen Tanvir'
date: 'Due Date: December 15, 2021, 5:59pm'
output:
  html_document:
    toc: yes
    number_sections: yes
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(readr)
library(modelr)
library(countrycode)
```

# Data collection

## What type of observational study are we conducting? 

Confirmatory - confirm or not confirm hypothesis derived from previous studies or hunches

Exploratory - search for explanatory variables that might be related to the response variable

If confirmatory, which explanatory variables are:

Primary variables - involved in the hypothesis

Control variables - included to reflect existing knowledge by accounting for known influences on the response variable

Acknowledgements

Reinhart, C., Rogoff, K., Trebesch, C. and Reinhart, V. (2019) Global Crises Data by Country.
[online] https://www.hbs.edu/behavioral-finance-and-financial-stability/data. Available at: https://www.hbs.edu/behavioral-finance-and-financial-stability/data/Pages/global.aspx [Accessed: 17 July 2019].

On this page we present data collected over many years by Carmen Reinhart (with her coauthors Ken Rogoff, Christoph Trebesch, and Vincent Reinhart). These include Banking Crisis dates for more than 70 countries from 1800-present, exchange rate crises, stock market crises, sovereign debt growth and default, and many other data series. 

To see Carmenâ€™s related research, please visit her website here:

https://carmenreinhart.com/debt-and-debt-crises/

https://carmenreinhart.com/wp-content/uploads/2020/02/229_data.pdf


The BFFS Project keeps this data updated and available for download. Please email us with any questions.

Inspiration

My inspiration stems from two questions: "Which factors are most associated with Systemic Crises in Africa?" And; "At which annual rate of inflation does an Inflation Crisis become a practical certainty?"

# Data preparation

## Import and transform into tidy data

```{r}
(ac <- 
   read_csv("reference/african_crises.csv", 
    col_types = cols(
        case = col_integer(), 
        cc3 = col_factor(ordered = TRUE), 
        country = col_character(),
        
        year = col_date(format = "%Y"),
        
        exch_usd = col_double(),
        gdp_weighted_default = col_double(), 
        inflation_annual_cpi = col_double(),
        
        systemic_crisis = col_logical(), 
        domestic_debt_in_default = col_logical(), 
        sovereign_external_debt_default = col_logical(), 
        independence = col_logical(), 
        currency_crises = col_integer(), 
        inflation_crises = col_logical(),
        
        banking_crisis = col_character()
    )
  ) 
  %>% mutate(banking_crisis = (banking_crisis == "crisis"))
  %>% mutate(currency_crises = (currency_crises > 0)) # assume 2==1 in 4 cases
)

assertthat::noNA(ac)
```

The data is tidy: Each variable has its own column, each observation has its own row, and each value has its own cell.

## The variables in `ac`

1. `case` is a factor variable that identifies the index for each country from the original, global dataset.  

2. `cc3` is a factor variable that identifies each country with a three letter code.

3. `country` is a factor variable that names each country.

4. `year` is a date variable that identifies the year of the observation.

5. `systemic_crisis` <logical> that identifies whether the observation is associated with a systemic crisis. 

6. `exch_usd` <dbl>

7. `domestic_debt_in_default` <int>

8. `sovereign_external_debt_default` <int>

9. `gdp_weighted_default` <dbl>

10. `inflation_annual_cpi` <dbl>

11. `independence` <logical>

12. `currency_crises` <logical>

13. `inflation_crises` <logical>

14. `banking_crisis` <logical>


# Web Scraping: Commodity Import & Export Price Index

From Commodity of Terms of Trade: Commodity Import/Export Price Index, Individual Commodites Weighted by Ratio of Imports to GDP Historical, Annual (1965-present), Fixed Weights, Index(2012=100)
```{r}
ac_imf = ac
```
In order to expand the features of our dataset, we scrape additional data provided by the International Monetary Fund (IMF). While the IMF provides many formats including API REST endpoints, excel and csv file formats but we instead scrape the rendered html using their [online table viewer](https://data.imf.org/?sk=2CDDCCB8-0B59-43E9-B6A0-59210D5605D2&sId=1390030341854).

Using the tool we selected the data desired and used inspect element to view the html source for the current page view. We then copied the div representing the entire table in html and pasted the text into local files.

```{r}
imf_html_to_df = function (html_filename){
  filename_pieces = str_split(html_filename, "_")[[1]]
  
  html_str = readLines(html_filename)
  html_to_byRowList = function(full_file_str){
    regex_exp = "<div class=\"PPTSCellConText.*?>(.*?)<"
    result = stringr::str_match_all(full_file_str, regex_exp)
    return(result)
  }
  
  # by row vector of all elements in our df
  elements = html_to_byRowList(html_str)[[1]][, 2] 
  
  # df 'schema' defined
  columns = c("country", seq(as.integer(filename_pieces[3]), as.integer(filename_pieces[4]), by=1))
  num_columns = length(columns)
  schema = matrix(ncol = num_columns, nrow = 0)
  df = data.frame(schema)
  colnames(df) = columns
  
  elements = elements[num_columns:length(elements)] # first elements are columns year labels (missing 'country')
  
  # populate df iterating through elements
  start = 1
  while(start + num_columns - 1 <= length(elements)){
    curr = elements[start:(start+num_columns-1)]
    curr = do.call(c, list(list(curr[1]), as.double(curr[2:(length(curr))])))
    df[nrow(df) + 1,] = curr
    start = start + num_columns
  }
  df$cc3 = as_factor(countrycode(df$country, origin = 'country.name', destination = 'iso3c')) 
  return(df)
}

export_df_1 = imf_html_to_df("reference/IMF_export_1965_1988_.html")
```
```{r, include=FALSE}
export_df_2 = imf_html_to_df("reference/IMF_export_1989_2014_.html")
export_df_3 = imf_html_to_df("reference/IMF_export_2015_2020_.html")
```
```{r}
export_df = merge(x=merge(x=export_df_1, y=export_df_2, by=c("cc3", "country")), y=export_df_3, by=c("cc3", "country"))
export_df = export_df %>% arrange(cc3)
as_tibble(export_df)
```

Transform the IMF  into a 'join compatable' data frame.
```{r}
imf_df_transform = function(df, col_name){
  just_cpi_df = subset(df, select = -c(cc3, country))
  cpi = unlist(just_cpi_df, use.names=FALSE)
  cc3 = unlist(subset(df, select = c(cc3)), use.names=FALSE)
  country = unlist(subset(df, select = c(country)), use.names=FALSE)
  year = colnames(just_cpi_df)
  row_repeats = length(cpi) / length(cc3)
  col_repeats = length(cpi) / length(year)
  as.date=function(x){
    year_starts = paste(as.character(x), rep("01", length(x)), rep("01", length(x)), sep="-" )
    return(as.Date(year_starts, format="%Y-%m-%d"))
  }
  df_joinable = data.frame(country=rep(country, times=row_repeats), cc3=rep(cc3, times=row_repeats), year=rep(as.date(year), each=col_repeats))
  df_joinable[col_name] = cpi
  df_joinable = subset (df_joinable, select = -country)
  return(df_joinable)
}
export_df_joinable = imf_df_transform(export_df, "inflation_annual_epi")
```
Lastly, we inner join the two tables!
```{r}
ac_imf = inner_join(ac_imf, export_df_joinable, by = c("cc3", "year"))
```


We then repeat the whole process to process webpages containing import price data
```{r}
import_df_1 = imf_html_to_df("reference/IMF_import_1965_1990_.html")
```
```{r, include=FALSE}
import_df_2 = imf_html_to_df("reference/IMF_import_1991_2016_.html")
import_df_3 = imf_html_to_df("reference/IMF_import_2017_2020_.html")
```
```{r}
import_df = merge(x=merge(x=import_df_1, y=import_df_2, by=c("cc3", "country")), y=import_df_3, by=c("cc3", "country"))
import_df = import_df %>% arrange(cc3)
as_tibble(import_df)

import_df_joinable = imf_df_transform(import_df, "inflation_annual_ipi")

# dont run this more than once 
ac_imf = inner_join(ac_imf, import_df_joinable, by = c("cc3", "year"))

as_tibble(ac_imf %>% select(country, year, inflation_annual_cpi, inflation_annual_epi, inflation_annual_ipi))
```

other ideas: rainfall, almanac/climate date

### Transform Index Annual to Index Percentage Change
```{r}
ac_imf = ac_imf %>% group_by(cc3) %>% arrange(year) %>% 
  mutate(inflation_change_cpi=(inflation_annual_cpi-lag(inflation_annual_cpi))/lag(inflation_annual_cpi)*100) %>%
  mutate(inflation_change_epi=(inflation_annual_epi-lag(inflation_annual_epi))/lag(inflation_annual_epi)*100) %>%
  mutate(inflation_change_ipi=(inflation_annual_ipi-lag(inflation_annual_ipi))/lag(inflation_annual_ipi)*100) %>% ungroup()

ac_imf = ac_imf %>% filter(!is.na(inflation_change_cpi))

as_tibble(ac_imf %>% arrange(cc3, year) %>% select(cc3, year, inflation_annual_cpi, inflation_change_cpi, inflation_annual_epi, inflation_change_epi, inflation_annual_ipi, inflation_change_ipi))
```


# Exploratory Data Analysis

```{r}
ac_eda

geom_point(mapping = aes(x = year, y = independence, color = country))

geom_point(mapping = aes(x = year, y = systemic_crisis, color = country))

# Jitter or predetermined distances
# TRUE values only
# ideological basis

plt +
#  geom_point(mapping = aes(x = banking_crisis))
  geom_jitter(mapping = aes(x = banking_crisis, y = systemic_crisis), width = .25, height = .25)

(indep_table <- df %>% 
  filter(independence == 1) %>%  
  group_by(country) %>% 
  summarize(indep_year = min(year), count = n() ) %>% 
  arrange(indep_year) %>% 
  mutate(prop = count/sum(count))
)

(sys_crisis_table <- df %>% 
  filter(systemic_crisis == 1) %>%  
  group_by(country) %>% 
  summarize(earliest_sys_crisis = min(year), count = n() ) %>% 
  arrange(earliest_sys_crisis) %>% 
  mutate(prop = count/sum(count))
)

# Add date of independence
```

Most countries achieved independence by 1965. The four exceptions are Algeria (1968) and Angola (1975) on the later end and Egypt (1860) and South Africa (1910) on the earlier end. Consequently, Algeria and Angola have less filtered observations for analysis, while Egypt and South Africa alone would make up a large proportion of the data used for analysis. The four countries combined makeup almost about 40% of the data, so let's use the proportion sizes to help us decide how to split the data for training/exploration and validation (query and test).

Only 82 total systemic crises in data, with the earliest occurring in Algeria in 1870 and in Egypt in 1907. Perhaps we should focus on analysis on the more recent period from 1976 and on, which also coincides with the time period when all countries had achieved independence, in order to train the model on data characterized by recent economic conditions under independence.

Combine independence and systemic crises into one plot. Identify marker for ind per country on its own line and then its systemic crises. Is systemic crisis associated with xxx? Demonstrate simple relationships with scatter plots and regression. That indicates hypothesis and additional variables to collect/create.Break out by countries.

```{r}
plt <- ggplot(data = ac, mapping = aes(y = systemic_crisis)) 

plt +
  geom_jitter(mapping = aes(x = banking_crisis, y = systemic_crisis), width = .25, height = .25)
```

# Preliminary model investigation

## Simple Logistic Regression (SLR) Model

The logistic response function:

```{r}
# Logistic response function
pi_i <- function(B0, B1, X) {
  return ( 1 / ( 1 + exp( - (B0 + B1 * X) ) ) )
}
```

The probability distribution for an observation is simply the probability mass function of a Bernoulli random variable. And since the Yi observations are assumed to be independent, their joint probability function is the likelihood.

```{r}

distr_i <- function(B0, B1, X, Y) {
  probs <- pi_i(B0, B1, X)
  return ( probs^Y * (1-probs)^(1-Y) ) 
}

likelihood <- function(B0, B1, X, Y) {
  return ( prod( distr_i(B0, B1, X, Y) ) )
}
```

However, it is numerically advantageous to find the maximum likelihood estimates by working with the logarithm of the joint probability function rather than the actual values which tend to be very close to zero and therefore prone to errors of computational precision. Therefore, the objective function to minimize is the log-likelihood:

```{r}

log_likelihood <- function(B0, B1, X, Y) {
  return ( sum( Y * log(pi_i(B0, B1, X))  + ( 1 - Y ) * log( 1 - pi_i(B0, B1, X) ) ) )
}
```

Create a helper function to use in the `optim()` function:

```{r}
ll_helper <- function(params) {
  return( -log_likelihood(params[1], params[2], X = X, Y = Y) )
  # Take the negative of the log-likelihood since we want to maximize this value but the optim() function minimizes by default.
}
```

Obtain point estimates of the coefficients of a simple logistic regression where the fitting is based on maximizing the log-likelihood of the data. 

```{r}

X = ac$systemic_crisis
Y = ac$banking_crisis
params <- c(0, 0)
(best <- optim(par = params, fn = ll_helper))

probs <- pi_i(best$par[1], best$par[2], X)
cutoff <- mean(unique(probs)) # only two values
preds <- ifelse(probs > cutoff, TRUE, FALSE)
correct <- preds == Y

paste("Training Accuracy", mean(correct))
paste("Sensitivity", mean(preds[Y==1] == Y[Y==1]))
paste("Sensitivity", mean(preds[Y==0] == Y[Y==0]))
paste("Correctly predicted",
  sum(preds & correct),
  "of",
  sum(Y),
  "crises"
)
paste("False positives",
  sum(preds & !(correct))

)
```

## Compare to built-in function for Logistic Regression as a family of the Generalized Linear Model

```{r}
glm_fit = glm(formula = Y ~ X, family = binomial(link = "logit"))

glm_probs <- predict(glm_fit, newdata = data.frame(Y, X), type = "response")

glm_preds <- ifelse(glm_probs > mean(unique(glm_probs)), TRUE, FALSE)

glm_correct <- glm_preds == Y

paste("Training Accuracy", mean(glm_correct))
paste("Sensitivity", mean(preds[Y==1] == Y[Y==1]))
paste("Sensitivity", mean(preds[Y==0] == Y[Y==0]))
paste("Correctly predicted",
  sum(glm_preds & glm_correct),
  "of",
  sum(Y),
  "crises"
)
paste("False positives",
  sum(glm_preds & !(glm_correct))
)
```
The default method "glm.fit" uses iteratively reweighted least squares (IWLS). 

## Compare likelihood from custom SLR vs. built-in GLM

```{r}
(slr_ll <- log_likelihood(best$par[1], best$par[2], X, Y))
(glm_ll <- log_likelihood(glm_fit$coefficients[1],    
                          glm_fit$coefficients[2], X, Y) )
```

The optimization procedure provides us with point estimates, but we have no measure of uncertainty for them: neither a standard deviation nor confidence intervals to determine whether or not they are different from 0 with any statistical certainty. The bootstrap method allows us to do so.

## Bootstrap confidence intervals

Construct a bootstrap sample for the estimated coefficients of a simple logistic regression where the fitting is based on maximizing the log-likelihood. 

```{r}
n <- nrow(Y)
M <- 1e4
b_params <- c(0, 0)
# fit_bstp <- vector(mode = "list", length = M)
b0 <- numeric(M)
b1 <- numeric(M)

max_ll_regression_helper_bstp <- function(b) {
  return( -log_likelihood(b[1], b[2], X = X, Y = Y) )
}

for (m in 1:M) {
  bstp <- sample(n, replace = TRUE)
  X <- X[bstp]
  Y <- Y[bstp]
  fit_bstp <- optim(b_params, max_ll_regression_helper_bstp)
  b0[m] <- fit_bstp$par[1]
  b1[m] <- fit_bstp$par[2]
}
```


## c. Estimate the standard deviations of each of the coefficients.

```{r}
b0_summary <- summary(b0)
b0_samp_var <- sum( (b0 - b0_summary["Mean"])^2 ) / ( length(b0) - 1 )
(b0_std_dev <- sqrt(b0_samp_var))
sd(b0)
```

```{r}
b1_summary <- summary(b1)
b1_samp_var <- sum( (b1 - b1_summary["Mean"])^2 ) / ( length(b1) - 1 )
(b1_std_dev <- sqrt(b1_samp_var))
sd(b1)
```


## d. Construct 95% confidence intervals for the coefficients and, based on those, determine whether or not we can reject a null hypothesis that each of them is equal to zero.

```{r}
alpha <- .05
probs <- c(alpha/2, 1 - alpha/2)
```

```{r}
( b0_empirical_bootstrap_CI <- quantile(b0, probs) )

ifelse(b0_empirical_bootstrap_CI[1] < 0 & 0 < b0_empirical_bootstrap_CI[2], "Fail to reject null", "Reject null")

ggplot(data.frame(x = b0)) +
  geom_histogram(aes(x = x, y = ..density..), color = "white") +
  geom_vline(xintercept = b0_empirical_bootstrap_CI, col = "blue")

```


```{r}
( b1_empirical_bootstrap_CI <- quantile(b1, probs) )

ifelse(b1_empirical_bootstrap_CI[1] < 0 & 0 < b1_empirical_bootstrap_CI[2], "Fail to reject null", "Reject null")


ggplot(data.frame(x = b1)) +
  geom_histogram(aes(x = x, y = ..density..), color = "white") +
  geom_vline(xintercept = b1_empirical_bootstrap_CI, col = "blue")
```


## Multiple Logistic Regression (MLR) Model

...Bootstrap confidence interval
compare to glm results

# General Linear Model Regression

```{r}
# some percents are Inf, can't perform regression
# assertthat::assert_that(all(is.finite(ac_imf$inflation_change_cpi)))
# ac_imf %>% filter(inflation_change_cpi==Inf)

# but we can perform logistic regression on the annual values
ac_imf = ac_imf %>% mutate(banking_crisis=as.factor(banking_crisis))
glm.fit = glm(formula = banking_crisis ~ inflation_annual_cpi + inflation_annual_epi + inflation_annual_ipi, family = binomial, data = ac_imf)

glm.probs <- predict(glm.fit, newdata = ac_imf, type = "response")

glm.pred <- ifelse(glm.probs > 0.5, TRUE, FALSE)

lr_got_it_right = glm.pred == as.logical(ac_imf$banking_crisis)

paste("Training Accuracy", mean(lr_got_it_right))
paste("Correctly predicted",
  sum(glm.pred & lr_got_it_right),
  "of",
  sum(as.logical(ac_imf$banking_crisis)),
  "crises"
)
paste("False positives",
  sum(glm.pred & !(lr_got_it_right))
)
```


In order to potentially capture any non-linearity in the data; we can apply a basis expansion and see if it improves performance.
```{r}
#library(grpreg)
#Data <- gen_nonlinear_data(n=1000)
input_features = ac_imf %>% select(inflation_annual_cpi, inflation_annual_epi, inflation_annual_ipi)
nrow = length(input_features$inflation_annual_cpi)
data = as.numeric(as.matrix(input_features))
X = matrix(data, ncol=3L, nrow=nrow)
X <- grpreg::expand_spline(X, df=6)$X
X = as.data.frame(X)
X$banking_crisis = ac_imf$banking_crisis

glm.fit = glm(formula = banking_crisis ~ V1_1 + V1_2 + V1_3 + V1_4 + V1_5 + V1_6 + V2_1 + V2_2 + V2_3 + V2_4 + V2_5 + V2_6 + V3_1 + V3_2 + V3_3 + V3_4 + V3_5 + V3_6, family = binomial, data = X)
glm.probs <- predict(glm.fit, newdata = X, type = "response")
glm.pred <- ifelse(glm.probs > 0.5, TRUE, FALSE)

lr_got_it_right = glm.pred == as.logical(ac_imf$banking_crisis)
paste("Training Accuracy", mean(lr_got_it_right))
paste("Correctly predicted",
  sum(glm.pred & lr_got_it_right),
  "of",
  sum(as.logical(ac_imf$banking_crisis)),
  "crises"
)
paste("False positives",
  sum(glm.pred & !(lr_got_it_right))
)



```
While we are able to predict 1 more crisis accurately, we also have 1 more false positive.
